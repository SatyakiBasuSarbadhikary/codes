{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Python has `map` and `reduce` built-in functions, we use it to demonstrate the concepts behind Map-Reduce. Then, we move forward to do some basic experiments using actual `map` and `reduce` functions in Spark.\n",
    "\n",
    "**Note:** This notebook is designed to run in Python 2. For Python 3 you may need to modify some parts of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Example (Square of a Vector)\n",
    "Suppose we have a vector (list) of numbers and we want to calculate the squares of each number of that vector. Let’s do it with a `for` loop first, and then without a for loop and only using a simple `map` function that does the job for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:24:59.230845Z",
     "iopub.status.busy": "2023-02-04T15:24:59.230391Z",
     "iopub.status.idle": "2023-02-04T15:24:59.242402Z",
     "shell.execute_reply": "2023-02-04T15:24:59.241425Z",
     "shell.execute_reply.started": "2023-02-04T15:24:59.230759Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:25:23.593213Z",
     "iopub.status.busy": "2023-02-04T15:25:23.592900Z",
     "iopub.status.idle": "2023-02-04T15:25:23.602719Z",
     "shell.execute_reply": "2023-02-04T15:25:23.601891Z",
     "shell.execute_reply.started": "2023-02-04T15:25:23.593171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers: [1, 2, 3, 4]\n",
      "1 X 1 = 1\n",
      "2 X 2 = 4\n",
      "3 X 3 = 9\n",
      "4 X 4 = 16\n",
      "squared: [1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "# define square function:\n",
    "def sqr(x): return x ** 2 \n",
    "\n",
    "# the traditional way using loop:\n",
    "numbers = [1, 2, 3, 4]\n",
    "squared = []\n",
    "\n",
    "print('numbers: ' + str(numbers)) # print thr input numbers\n",
    "\n",
    "for n in numbers: # loop\n",
    "    squared.append(sqr(n)) # save the results\n",
    "    print (str(n) + \" X \" + str(n) + \" = \" + str(squared[-1])) # print n and the last element of squared\n",
    "    \n",
    "print('squared: ' + str(squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:26:50.561670Z",
     "iopub.status.busy": "2023-02-04T15:26:50.561320Z",
     "iopub.status.idle": "2023-02-04T15:26:50.573778Z",
     "shell.execute_reply": "2023-02-04T15:26:50.573022Z",
     "shell.execute_reply.started": "2023-02-04T15:26:50.561626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f797ce4cbd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then map the function onto the list\n",
    "map(sqr, numbers) # note the different syntax/usage for a single input: e.g. sqr(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy! We observe that `map` applies `sqr` function on every elements of the input list (i.e., `numbers`).\n",
    "\n",
    "Because map expects a function to be passed in, it also happens to be one of the places where `lambda` routinely appears. In fact, `lambda` (aka anonymous function) helps us to define functions on the fly without the need to be bounded to a name. This feature is very helpful when we do not need to call a function in different part of a code script (more reading: http://www.secnetix.de/olli/Python/lambda_functions.hawk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:27:35.432555Z",
     "iopub.status.busy": "2023-02-04T15:27:35.432127Z",
     "iopub.status.idle": "2023-02-04T15:27:35.441111Z",
     "shell.execute_reply": "2023-02-04T15:27:35.440105Z",
     "shell.execute_reply.started": "2023-02-04T15:27:35.432499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f797ce4cb50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map((lambda x: x **2), numbers) # mapping with a lmbda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Example (Summation)\n",
    "Recall from Example 1 that `map` gets a list, maps a function on all of the elements of the list and returns the result which is another list. In opposite, `reduce` gets a list, apply a function of its elements, but returns the result as a single number (not a list). For example, we can use `reduce` to compute sum of the elements of a list. Let’s see `reduce` in action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:27:44.657536Z",
     "iopub.status.busy": "2023-02-04T15:27:44.657203Z",
     "iopub.status.idle": "2023-02-04T15:27:44.668681Z",
     "shell.execute_reply": "2023-02-04T15:27:44.667881Z",
     "shell.execute_reply.started": "2023-02-04T15:27:44.657494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = range(1,5)\n",
    "reduce((lambda x, y: x + y), numbers) # reduce 1,2,3,4 : 1 + 2 + 3 + 4 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why 10? Because 1 + 2 + 3 + 4 = 10.\n",
    "\n",
    "Note that in the above statement, `x` and `y` are initially the first two elements of the list `numbers`. Then as they move along the data, `x` becomes the sum of the previous numbers while `y` indicates next element in the list. \n",
    "\n",
    "Here is the equivalent with a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:28:16.931530Z",
     "iopub.status.busy": "2023-02-04T15:28:16.931186Z",
     "iopub.status.idle": "2023-02-04T15:28:16.937444Z",
     "shell.execute_reply": "2023-02-04T15:28:16.936617Z",
     "shell.execute_reply.started": "2023-02-04T15:28:16.931487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = numbers[0] # grab the first\n",
    "for y in numbers[1:]: # or use range(1, len(numbers) + 1)\n",
    "    x = x + y # multiply by the second and update\n",
    "    \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine all `lambda`, `map`, and `reduce` to calculate the sum of the squares of the elements of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:28:31.911043Z",
     "iopub.status.busy": "2023-02-04T15:28:31.910712Z",
     "iopub.status.idle": "2023-02-04T15:28:31.923082Z",
     "shell.execute_reply": "2023-02-04T15:28:31.921908Z",
     "shell.execute_reply.started": "2023-02-04T15:28:31.911001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = range(1,5)\n",
    "reduce((lambda x, y: x+ y), map((lambda x: x **2), numbers)) # 1 + 4 + 9 + 16 = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Data\n",
    "Let's start with loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-04T15:29:13.452364Z",
     "iopub.status.busy": "2023-02-04T15:29:13.452055Z",
     "iopub.status.idle": "2023-02-04T15:31:45.055237Z",
     "shell.execute_reply": "2023-02-04T15:31:45.053909Z",
     "shell.execute_reply.started": "2023-02-04T15:29:13.452325Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function # this version is Py2: print vs print()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SparkConf\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a SparkContext is an object that represents a connection to a Spark cluster. We use this object to create Resilient Distributed Datasets (RDD) and broadcast variables on that cluster.\n",
    "\n",
    "Before going further, let's make sure there is no Spark Context running in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sc.stop() # sometimes you have to stop previous 'context', especially if it crashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize Spark and set a name for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=SparkConf(), appName='PyWordCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a text file `spark.txt`. The function `textFile` reads and submits a text file to Spark. Indeed, each line of the text file is considered as one independent element. The `textFile` function also splits and distributes the data over some partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#path = os.path.join(\"/home/sourabh/DURBA_SYLLABUS/DATA_SCIENCE_SYLLABUS/BIGDATA/PYDOOP\")\n",
    "inputRDD = sc.textFile(\"spark.txt\")\n",
    "print(inputRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the number of partitions and the size of RDD (the total number of elements) we can simply call `getNumPartitions` and `count` methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('The number of partitions: ',inputRDD.getNumPartitions(),\n",
    "#      '\\nThe total number of elements: ', inputRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputRDD.mapPartitions(lambda m: [1]).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputRDD.map(lambda m: len(m)).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, let's take a look at the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you students? Today is particularly hot, and learning Hadoop and Spark may not be\r\n",
      "easy! But try to focus anyway, and we shall learn a lot about Hadoop and Spark, using Python.\r\n",
      "We shall also learn how to handle very large data consisting of 111 million entries.\r\n"
     ]
    }
   ],
   "source": [
    "! cat spark.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings\n",
    "Here we use three built-in mapping methods from Spark: `map`, `flatMap`, and `mapPartitions`. The `map` returns a new RDD by applying a function to each element of the input RDD. Therefore, for each single element in input RDD we will have exactly one element in output RDD (1-to-1 mapping).\n",
    "The `flatMap` applies a function on all elements, then returns the flattened results. This means we may have a list of returned results for each single input (1-to-N mapping). Finally, the `mapPartitions` applies a function on every partitions (not elements) of the RDD.\n",
    "\n",
    "Let's do some simple expriments to understand the differences between `map` and `mapPartisions` (which we will extensively use in the this and other activities). Suppose we want to count the number of elements (in this example the number of lines) in our RDD. To find out this number, we map every elements to a single value `1`. Then we sum up all these `1`s which gives us the total number of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputRDD.map(lambda m: 1).reduce(lambda a,b: a+b) # = inputRDD.inputRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of partitions, we redo the above expression but using `mapPartitions`. Indeed, we map each partition into a partition which only has one `[1]` and then sum up these values. Note that we have to use `[1]` instead of `1` as the partitions has to be an iterator (i.e., lists as opposed to single values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputRDD.mapPartitions(lambda m: [1]).reduce(lambda a,b: a+b) # = inputRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1:** How we can count the number of characters in the text file using `map` and `reduce` functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our main problem, we intend to calculate the frequency of each word in the document. Therefore, we use `flatMap` to split the text file into single words (so the input elements are lines and the outputs are lists of words in each line). Then we use `map` to create tuples of the form `<key, value>`. Here, `key` is nothing but a single word that was previously generated by `flatMap`, and the `value`s are always `1`.\n",
    "\n",
    "One may ask \"*why we set all the values equal to 1 regardless of their keys?*\", or even \"*why we need such tuples, anyway?*\". The answer is that the key-value tuples are the common way of intraction between the \"distributed mapers\" and the \"central reducer\". Indeed, the logic behind having this special mapping in this example is that every single word that a mapper receives is counted as one more observation. Therefore the value of that key should be `1`. In the final stage, the reducer groups all tuples with the same keys and then adds their values. Therefore, if a certain word has been seen for $N$ times (no matter how many mapper we had), the reducer gets $N$ tuples with that specfic word as the key. Ironically, the sum of $N$ `1`s is nothing but $N$ which is exactly the number of occurrence of that word. \n",
    "\n",
    "Now that we know the logic behind the mapping and reducing, let's do the splitting to get the list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[6] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "words = inputRDD.flatMap(lambda x: x.split(' '))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then generating < word, 1> tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "wordsOne = words.map(lambda x: (x, 1))\n",
    "print(wordsOne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing\n",
    "As mentioned before, our reducer only need to group the < word, 1 > tuples by their keys and add their values to calculate the number of times that word was observed in the text file. To do so, we use `reduceByKy` method (instead of `reduce` which ignores keys), and declare we want to calculate the sum of the values by seting its input parameter as `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[12] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "wordCounts = wordsOne.reduceByKey(add)\n",
    "print(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting\n",
    "Now, the only thing remians is to collect the result. Indeed, up to this point Spark did not touched our data! By calling `collect` method, Spark does all the above steps and returns the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wordCounts.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 1),\n",
       " ('students?', 1),\n",
       " ('Today', 1),\n",
       " ('is', 1),\n",
       " ('particularly', 1),\n",
       " ('learning', 1),\n",
       " ('Spark', 1),\n",
       " ('may', 1),\n",
       " ('easy!', 1),\n",
       " ('But', 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the 10 first entries of the result\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the output entries have no specific order (e.g., alphabetically, order of occurrence or frequency of occurrence) because behind the scene Spark shuffles the inputs of the reducer(s). \n",
    "\n",
    "To see the top ten frequent words, we can sort the output elements and print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 4),\n",
       " ('Hadoop', 2),\n",
       " ('to', 2),\n",
       " ('shall', 2),\n",
       " ('learn', 2),\n",
       " ('are', 1),\n",
       " ('students?', 1),\n",
       " ('Today', 1),\n",
       " ('is', 1),\n",
       " ('particularly', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(output, key=lambda x: x[1], reverse = True)[0:10] # top 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact Version\n",
    "Using the power of piplining, we could do all the above steps in a very compact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 4),\n",
       " ('Hadoop', 2),\n",
       " ('to', 2),\n",
       " ('shall', 2),\n",
       " ('are', 1),\n",
       " ('students?', 1),\n",
       " ('Today', 1),\n",
       " ('is', 1),\n",
       " ('particularly', 1),\n",
       " ('learning', 1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop() # stop previous 'context'\n",
    "sc = SparkContext(appName=\"PythonWordCount\") # create a new context\n",
    "# the following does all the job at once:\n",
    "output = sc.textFile(\"./spark.txt\").flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(add).collect()\n",
    "# and the output:\n",
    "sorted(output, key=lambda x: x[1], reverse = True)[0:10] # top 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
